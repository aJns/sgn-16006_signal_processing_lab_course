\section{Methods}

\subsection{Feature extraction}
We load the audio signals data using the Python module soundfile. This command
gives us the actual data and the sample rate used. The sample rate in all the
data was 8000Hz.

The features we want are the relative energy ratios of 4 frequency bands:
\(0.0 - 0.5 kHz\), \(0.5 - 1.0 kHz\), \(1.0 - 2.0 kHz\) and \(2.0 - 4.0 kHz\).
We get this by first dividing the audio signal into frames with a length of
30ms, and an overlap of 15ms. With the sampling rate used this means that one
frame has 240 samples.

These frames are then multiplied, or windowed using the Hanning window
function. In figure~\ref{fig:hann50} there's the waveform of the 50th windowed
frame.
\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{50-hann-window}
  \caption{50th Hanning-windowed frame}
\label{fig:hann50}
\end{figure}

Then we get the discrete fourier transform of the windowed frames,
with a bin size of 1024. In figure~\ref{fig:amp_spec} there's the amplitude
spectrum of the 50th frame.
\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{50-hann-window-fft}
  \caption{Amplitude spectrum of the 50th frame}
\label{fig:amp_spec}
\end{figure}

Out of the frame's DFT we extract the values corresponding to the 4 frequency
bands. For example, the indices of the frequency band \(1-2 kHz\) are 128 to
256. Then we calculate the energy ratio of each band. The energy ratio is
calculated by dividing the energy of a subband by the total energy of that
frame. In figure~\ref{fig:features} there's a visualization of the 50th frames
extracted features.
\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{50-frame-features}
  \caption{Features of the 50th frame}
\label{fig:features}
\end{figure}
Finally, we combine all the frames of a sample by taking the average of all the
frames energy ratios.


\subsection{k-Nearest neighbor classification}

When we have extracted the features, we implement a k-nn classifier as a Python
object. Basically, in the training phase we just copy the data and labels. The
real work is done in the prediction function, where we calculate all the
distances between the sample whose label we're predicting, and all the training
samples. When we have the distances, we take the k-lowest (ie.\ nearest), and
out of those samples we take the most common.

The data set we use in the classification is the \textit{Environmental Noise
Data Set (Series 2)} recorded by the University of East Anglia. In the test
code and results we refer classes by their index number, but in
table~\ref{tab:classes} we've compiled the class names and numbers together.
\begin{table}
  \centering
  \caption{Class names and the corresponding numbers}
  \begin{tabular}{lc}  
    \toprule
    \cmidrule(r){1-2}
    Class name    & Class number \\
    \midrule

    Buildingsite & 1   \\
    Bus & 2   \\
    Car & 3   \\
    Car highway & 4   \\
    Laundrette & 5   \\
    Office & 6   \\
    Presentation & 7   \\
    Shopping centre & 8   \\
    Street people & 9   \\
    Street traffic & 10   \\
    Supermarket & 11   \\
    Train & 12   \\

    \bottomrule
  \end{tabular}
\label{tab:classes}
\end{table}

First we classify using only one neighbor. The overall accuracy was reported by
scikit-learn as 72.1\%.
Table~\ref{tab:accuracy_k1} has the prediction accuracies of the individual
classes.
\begin{table}
  \centering
  \caption{The prediction accuracies of the NN classifier when k=1}
  \begin{tabular}{cc}  
    \toprule
    \cmidrule(r){1-2}
    Class    & Classification accuracy (\%) \\
    \midrule

    1	& 98.46  \\
    2	& 50.00  \\
    3	& 77.78  \\
    4	& 79.69  \\
    5	& 74.24  \\
    6	& 100.00  \\
    7	& 73.13  \\
    8	& 52.46  \\
    9	& 50.94  \\
    10	& 77.78  \\
    11	& 46.03  \\
    12	& 78.26  \\

    \bottomrule
  \end{tabular}
\label{tab:accuracy_k1}
\end{table}


In figure~\ref{fig:conf_mat} we can see the color plot
of the normalized confusion matrix.
\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{conf_matrix}
  \caption{Confusion matrix of k=1 classifier}
\label{fig:conf_mat}
\end{figure}


Then we run the classifier with 5 neighbors. This time the accuracy was 73.5\%.
Table~\ref{tab:accuracy_k5} has the prediction accuracies of the individual
classes.
\begin{table}
  \centering
  \caption{The prediction accuracies of the NN classifier when k=5}
  \begin{tabular}{cc}  
    \toprule
    \cmidrule(r){1-2}
    Class    & Classification accuracy (\%) \\
    \midrule

    1	& 100.00  \\
    2	& 53.85  \\
    3	& 88.89  \\
    4	& 84.38  \\
    5	& 80.30  \\
    6	& 100.00  \\
    7	& 67.16  \\
    8	& 54.10  \\
    9	& 54.72  \\
    10	& 74.07  \\
    11	& 39.68  \\
    12	& 84.06  \\

    \bottomrule
  \end{tabular}
\label{tab:accuracy_k5}
\end{table}

Figure~\ref{fig:conf_mat_k5} has the color plot of the normalized confusion
matrix of that classifier.
\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{conf_matrix_k5}
  \caption{Confusion matrix of k=5 classifier}
\label{fig:conf_mat_k5}
\end{figure}









